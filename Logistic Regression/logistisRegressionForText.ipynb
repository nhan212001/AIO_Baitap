{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sentiment_analysis.csv\", index_col=\"id\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_normalize(text):\n",
    "    # Retweet old acronym \"RT\" removal\n",
    "    text = re.sub(r\"^RT[\\s]+\", \"\", text)\n",
    "\n",
    "    # Hyperlinks removal\n",
    "    text = re.sub(r\"https?://.*[\\r\\n]*\", \"\", text)\n",
    "\n",
    "    # Hashtags removal\n",
    "    text = re.sub(r\"#\", \"\", text)\n",
    "\n",
    "    # Punctuation removal\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    # Tokenization\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    text_tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    return text_tokens\n",
    "\n",
    "\n",
    "def get_freqs(df):\n",
    "    freqs = defaultdict(lambda: 0)\n",
    "    for idx, row in df.iterrows():\n",
    "        tweet = row[\"tweet\"]\n",
    "        label = row[\"label\"]\n",
    "        tokens = text_normalize(tweet)\n",
    "        for token in tokens:\n",
    "            pair = (token, label)\n",
    "            freqs[pair] += 1\n",
    "    return freqs\n",
    "\n",
    "\n",
    "def get_feature(text, freqs):\n",
    "    tokens = text_normalize(text)\n",
    "    X = np.zeros(3)\n",
    "    X[0] = 1\n",
    "    for token in tokens:\n",
    "        X[1] += freqs[(token, 0)]\n",
    "        X[2] += freqs[(token, 1)]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "freqs = get_freqs(df)\n",
    "for idx, row in df.iterrows():\n",
    "    tweet = row[\"tweet\"]\n",
    "    label = row[\"label\"]\n",
    "\n",
    "    X_i = get_feature(tweet, freqs)\n",
    "    X.append(X_i)\n",
    "    y.append(label)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 0.2\n",
    "test_size = 0.125\n",
    "random_state = 2\n",
    "is_shuffle = True\n",
    "\n",
    "# First split: train+val and test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=random_state, shuffle=is_shuffle\n",
    ")\n",
    "\n",
    "# Second split: train and val\n",
    "val_ratio = val_size / (1 - test_size)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=val_ratio, random_state=random_state, shuffle=is_shuffle\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = StandardScaler()\n",
    "X_train[:, 1:] = normalizer.fit_transform(X_train[:, 1:])\n",
    "X_val[:, 1:] = normalizer.transform(X_val[:, 1:])\n",
    "X_test[:, 1:] = normalizer.transform(X_test[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def compute_loss(y_hat, y):\n",
    "    # Đảm bảo y_hat nằm trong khoảng [1e-7, 1 - 1e-7] để tránh log(0)\n",
    "    y_hat = np.clip(y_hat, 1e-7, 1 - 1e-7)\n",
    "    return (-y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)).mean()\n",
    "\n",
    "\n",
    "def predict(X, theta):\n",
    "    dot_product = np.dot(X, theta)\n",
    "    return sigmoid(dot_product)\n",
    "\n",
    "\n",
    "def compute_gradient(X, y, y_hat):\n",
    "    return np.dot(X.T, (y_hat - y)) / y.size\n",
    "\n",
    "\n",
    "def update_theta(theta, gradient, lr):\n",
    "    return theta - lr * gradient\n",
    "\n",
    "\n",
    "def compute_accuracy(y_hat, y):\n",
    "    return np.mean(np.round(y_hat) == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 200\n",
    "batch_size = 128\n",
    "\n",
    "np.random.seed(random_state)\n",
    "theta = np.random.uniform(size=X_train.shape[1])\n",
    "\n",
    "\n",
    "train_accs = []\n",
    "train_losses = []\n",
    "val_accs = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_batch_losses = []\n",
    "    train_batch_accuracies = []\n",
    "    val_batch_losses = []\n",
    "    val_batch_accuracies = []\n",
    "\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        X_batch = X_train[i : i + batch_size]\n",
    "        y_batch = y_train[i : i + batch_size]\n",
    "\n",
    "        y_hat = predict(X_batch, theta)\n",
    "        loss = compute_loss(y_hat, y_batch)\n",
    "        train_batch_losses.append(loss)\n",
    "\n",
    "        accuracy = compute_accuracy(y_hat, y_batch)\n",
    "        train_batch_accuracies.append(accuracy)\n",
    "\n",
    "        gradient = compute_gradient(X_batch, y_batch, y_hat)\n",
    "        theta = update_theta(theta, gradient, lr)\n",
    "\n",
    "        # Validation\n",
    "        y_val_hat = predict(X_val, theta)\n",
    "        val_loss = compute_loss(y_val_hat, y_val)\n",
    "        val_batch_losses.append(val_loss)\n",
    "        val_accuracy = compute_accuracy(y_val_hat, y_val)\n",
    "        val_batch_accuracies.append(val_accuracy)\n",
    "\n",
    "    train_losses.append(np.mean(train_batch_losses))\n",
    "    train_accs.append(np.mean(train_batch_accuracies))\n",
    "    val_losses.append(np.mean(val_batch_losses))\n",
    "    val_accs.append(np.mean(val_batch_accuracies))\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1}/{epochs} - \"\n",
    "        f\"Train Loss: {train_losses[-1]:.4f}, \"\n",
    "        f\"Train Accuracy: {train_accs[-1]:.4f}, \"\n",
    "        f\"Validation Loss: {val_losses[-1]:.4f}, \"\n",
    "        f\"Validation Accuracy: {val_accs[-1]:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "ax[0, 0].plot(train_losses)\n",
    "ax[0, 0].set(xlabel=\"Epoch\", ylabel=\"Loss\")\n",
    "ax[0, 0].set_title(\"Training Loss\")\n",
    "\n",
    "ax[0, 1].plot(val_losses, color=\"orange\")\n",
    "ax[0, 1].set(xlabel=\"Epoch\", ylabel=\"Loss\")\n",
    "ax[0, 1].set_title(\"Validation Loss\")\n",
    "\n",
    "ax[1, 0].plot(train_accs)\n",
    "ax[1, 0].set(xlabel=\"Epoch\", ylabel=\"Accuracy\")\n",
    "ax[1, 0].set_title(\"Training Accuracy\")\n",
    "\n",
    "ax[1, 1].plot(val_accs, color=\"orange\")\n",
    "ax[1, 1].set(xlabel=\"Epoch\", ylabel=\"Accuracy\")\n",
    "ax[1, 1].set_title(\"Validation Accuracy\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set_acc = compute_accuracy(predict(X_val, theta), y_val)\n",
    "test_set_acc = compute_accuracy(predict(X_test, theta), y_test)\n",
    "print(\"Evaluation on validation and test set:\")\n",
    "print(f\"Validation Accuracy: {val_set_acc}\")\n",
    "print(f\"Test Accuracy: {test_set_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
